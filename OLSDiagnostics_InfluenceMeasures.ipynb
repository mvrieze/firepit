{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Influence and Outlier Measures\n",
    "\n",
    "Created on Sun Jan 29 11:16:09 2012\n",
    "\n",
    "Author: Josef Perktold\n",
    "License: BSD-3\n",
    "\"\"\"\n",
    "from statsmodels.compat.python import lzip\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools.decorators import cache_readonly\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.tools.tools import maybe_unwrap_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers test convenience wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outlier_test(model_results, method='bonf', alpha=.05, labels=None,\n",
    "                 order=False):\n",
    "    \"\"\"\n",
    "    Outlier Tests for RegressionResults instances.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_results : RegressionResults instance\n",
    "        Linear model results\n",
    "    method : str\n",
    "        - `bonferroni` : one-step correction\n",
    "        - `sidak` : one-step correction\n",
    "        - `holm-sidak` :\n",
    "        - `holm` :\n",
    "        - `simes-hochberg` :\n",
    "        - `hommel` :\n",
    "        - `fdr_bh` : Benjamini/Hochberg\n",
    "        - `fdr_by` : Benjamini/Yekutieli\n",
    "        See `statsmodels.stats.multitest.multipletests` for details.\n",
    "    alpha : float\n",
    "        familywise error rate\n",
    "    order : bool\n",
    "        Whether or not to order the results by the absolute value of the\n",
    "        studentized residuals. If labels are provided they will also be sorted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    table : ndarray or DataFrame\n",
    "        Returns either an ndarray or a DataFrame if labels is not None.\n",
    "        Will attempt to get labels from model_results if available. The\n",
    "        columns are the Studentized residuals, the unadjusted p-value,\n",
    "        and the corrected p-value according to method.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The unadjusted p-value is stats.t.sf(abs(resid), df) where\n",
    "    df = df_resid - 1.\n",
    "    \"\"\"\n",
    "    from scipy import stats # lazy import\n",
    "    infl = getattr(model_results, 'get_influence', None)\n",
    "    if infl is None:\n",
    "        results = maybe_unwrap_results(model_results)\n",
    "        raise AttributeError(\"model_results object %s does not have a \"\n",
    "                \"get_influence method.\" % results.__class__.__name__)\n",
    "    resid = infl().resid_studentized_external\n",
    "    if order:\n",
    "        idx = np.abs(resid).argsort()[::-1]\n",
    "        resid = resid[idx]\n",
    "        if labels is not None:\n",
    "            labels = np.array(labels)[idx].tolist()\n",
    "    df = model_results.df_resid - 1\n",
    "    unadj_p = stats.t.sf(np.abs(resid), df) * 2\n",
    "    adj_p = multipletests(unadj_p, alpha=alpha, method=method)\n",
    "\n",
    "    data = np.c_[resid, unadj_p, adj_p[1]]\n",
    "    if labels is None:\n",
    "        labels = getattr(model_results.model.data, 'row_labels', None)\n",
    "    if labels is not None:\n",
    "        from pandas import DataFrame\n",
    "        return DataFrame(data,\n",
    "                         columns=['student_resid', 'unadj_p', method+\"(p)\"],\n",
    "                         index=labels)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence Measures\n",
    "## Ramsey Reset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_ramsey(res, degree=5):\n",
    "    '''Ramsey's RESET specification test for linear models\n",
    "\n",
    "    This is a general specification test, for additional non-linear effects\n",
    "    in a model.\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The test fits an auxiliary OLS regression where the design matrix, exog,\n",
    "    is augmented by powers 2 to degree of the fitted values. Then it performs\n",
    "    an F-test whether these additional terms are significant.\n",
    "\n",
    "    If the p-value of the f-test is below a threshold, e.g. 0.1, then this\n",
    "    indicates that there might be additional non-linear effects in the model\n",
    "    and that the linear model is mis-specified.\n",
    "\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    http://en.wikipedia.org/wiki/Ramsey_RESET_test\n",
    "\n",
    "    '''\n",
    "    order = degree + 1\n",
    "    k_vars = res.model.exog.shape[1]\n",
    "    #vander without constant and x:\n",
    "    y_fitted_vander = np.vander(res.fittedvalues, order)[:, :-2] #drop constant\n",
    "    exog = np.column_stack((res.model.exog, y_fitted_vander))\n",
    "    res_aux = OLS(res.model.endog, exog).fit()\n",
    "    #r_matrix = np.eye(degree, exog.shape[1], k_vars)\n",
    "    r_matrix = np.eye(degree-1, exog.shape[1], k_vars)\n",
    "    #df1 = degree - 1\n",
    "    #df2 = exog.shape[0] - degree - res.df_model  (without constant)\n",
    "    return res_aux.f_test(r_matrix) #, r_matrix, res_aux\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence Measures\n",
    "## Variance Inflation Factor - VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variance_inflation_factor(exog, exog_idx):\n",
    "    '''variance inflation factor, VIF, for one exogenous variable\n",
    "\n",
    "    The variance inflation factor is a measure for the increase of the\n",
    "    variance of the parameter estimates if an additional variable, given by\n",
    "    exog_idx is added to the linear regression. It is a measure for\n",
    "    multicollinearity of the design matrix, exog.\n",
    "\n",
    "    One recommendation is that if VIF is greater than 5, then the explanatory\n",
    "    variable given by exog_idx is highly collinear with the other explanatory\n",
    "    variables, and the parameter estimates will have large standard errors\n",
    "    because of this.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exog : ndarray, (nobs, k_vars)\n",
    "        design matrix with all explanatory variables, as for example used in\n",
    "        regression\n",
    "    exog_idx : int\n",
    "        index of the exogenous variable in the columns of exog\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vif : float\n",
    "        variance inflation factor\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function does not save the auxiliary regression.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    xxx : class for regression diagnostics  TODO: doesn't exist yet\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    http://en.wikipedia.org/wiki/Variance_inflation_factor\n",
    "\n",
    "    '''\n",
    "    k_vars = exog.shape[1]\n",
    "    x_i = exog[:, exog_idx]\n",
    "    mask = np.arange(k_vars) != exog_idx\n",
    "    x_noti = exog[:, mask]\n",
    "    r_squared_i = OLS(x_i, x_noti).fit().rsquared\n",
    "    vif = 1. / (1. - r_squared_i)\n",
    "    return vif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence Measures\n",
    "## OLS Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OLSInfluence(object):\n",
    "    '''class to calculate outlier and influence measures for OLS result\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : Regression Results instance\n",
    "        currently assumes the results are from an OLS regression\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    One part of the results can be calculated without any auxiliary regression\n",
    "    (some of which have the `_internal` postfix in the name. Other statistics\n",
    "    require leave-one-observation-out (LOOO) auxiliary regression, and will be\n",
    "    slower (mainly results with `_external` postfix in the name).\n",
    "    The auxiliary LOOO regression only the required results are stored.\n",
    "\n",
    "    Using the LOO measures is currently only recommended if the data set\n",
    "    is not too large. One possible approach for LOOO measures would be to\n",
    "    identify possible problem observations with the _internal measures, and\n",
    "    then run the leave-one-observation-out only with observations that are\n",
    "    possible outliers. (However, this is not yet available in an automized way.)\n",
    "\n",
    "    This should be extended to general least squares.\n",
    "\n",
    "    The leave-one-variable-out (LOVO) auxiliary regression are currently not\n",
    "    used.\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, results):\n",
    "        #check which model is allowed\n",
    "        self.results = maybe_unwrap_results(results)\n",
    "        self.nobs, self.k_vars = results.model.exog.shape\n",
    "        self.endog = results.model.endog\n",
    "        self.exog = results.model.exog\n",
    "        self.model_class = results.model.__class__\n",
    "\n",
    "        self.sigma_est = np.sqrt(results.mse_resid)\n",
    "\n",
    "        self.aux_regression_exog = {}\n",
    "        self.aux_regression_endog = {}\n",
    "\n",
    "    @cache_readonly\n",
    "    def hat_matrix_diag(self):\n",
    "        '''(cached attribute) diagonal of the hat_matrix for OLS\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        temporarily calculated here, this should go to model class\n",
    "        '''\n",
    "        return (self.exog * self.results.model.pinv_wexog.T).sum(1)\n",
    "\n",
    "    @cache_readonly\n",
    "    def resid_press(self):\n",
    "        '''(cached attribute) PRESS residuals\n",
    "        '''\n",
    "        hii = self.hat_matrix_diag\n",
    "        return self.results.resid / (1 - hii)\n",
    "\n",
    "    @cache_readonly\n",
    "    def influence(self):\n",
    "        '''(cached attribute) influence measure\n",
    "\n",
    "        matches the influence measure that gretl reports\n",
    "        u * h / (1 - h)\n",
    "        where u are the residuals and h is the diagonal of the hat_matrix\n",
    "        '''\n",
    "        hii = self.hat_matrix_diag\n",
    "        return self.results.resid * hii / (1 - hii)\n",
    "\n",
    "    @cache_readonly\n",
    "    def hat_diag_factor(self):\n",
    "        '''(cached attribute) factor of diagonal of hat_matrix used in influence\n",
    "\n",
    "        this might be useful for internal reuse\n",
    "        h / (1 - h)\n",
    "        '''\n",
    "        hii = self.hat_matrix_diag\n",
    "        return hii / (1 - hii)\n",
    "\n",
    "    @cache_readonly\n",
    "    def ess_press(self):\n",
    "        '''(cached attribute) error sum of squares of PRESS residuals\n",
    "        '''\n",
    "        return np.dot(self.resid_press, self.resid_press)\n",
    "\n",
    "    @cache_readonly\n",
    "    def resid_studentized_internal(self):\n",
    "        '''(cached attribute) studentized residuals using variance from OLS\n",
    "\n",
    "        this uses sigma from original estimate\n",
    "        does not require leave one out loop\n",
    "        '''\n",
    "        return self.get_resid_studentized_external(sigma=None)\n",
    "        #return self.results.resid / self.sigma_est\n",
    "\n",
    "    @cache_readonly\n",
    "    def resid_studentized_external(self):\n",
    "        '''(cached attribute) studentized residuals using LOOO variance\n",
    "\n",
    "        this uses sigma from leave-one-out estimates\n",
    "\n",
    "        requires leave one out loop for observations\n",
    "        '''\n",
    "        sigma_looo = np.sqrt(self.sigma2_not_obsi)\n",
    "        return self.get_resid_studentized_external(sigma=sigma_looo)\n",
    "\n",
    "    def get_resid_studentized_external(self, sigma=None):\n",
    "        '''calculate studentized residuals\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigma : None or float\n",
    "            estimate of the standard deviation of the residuals. If None, then\n",
    "            the estimate from the regression results is used.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        stzd_resid : ndarray\n",
    "            studentized residuals\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        studentized residuals are defined as ::\n",
    "\n",
    "           resid / sigma / np.sqrt(1 - hii)\n",
    "\n",
    "        where resid are the residuals from the regression, sigma is an\n",
    "        estimate of the standard deviation of the residuals, and hii is the\n",
    "        diagonal of the hat_matrix.\n",
    "\n",
    "        '''\n",
    "        hii = self.hat_matrix_diag\n",
    "        if sigma is None:\n",
    "            sigma2_est = self.results.mse_resid\n",
    "            #can be replace by different estimators of sigma\n",
    "            sigma = np.sqrt(sigma2_est)\n",
    "\n",
    "        return  self.results.resid / sigma / np.sqrt(1 - hii)\n",
    "\n",
    "    @cache_readonly\n",
    "    def dffits_internal(self):\n",
    "        '''(cached attribute) dffits measure for influence of an observation\n",
    "\n",
    "        based on resid_studentized_internal\n",
    "        uses original results, no nobs loop\n",
    "\n",
    "        '''\n",
    "        #TODO: do I want to use different sigma estimate in\n",
    "        #      resid_studentized_external\n",
    "        # -> move definition of sigma_error to the __init__\n",
    "        hii = self.hat_matrix_diag\n",
    "        dffits_ = self.resid_studentized_internal * np.sqrt(hii / (1 - hii))\n",
    "        dffits_threshold = 2 * np.sqrt(self.k_vars * 1. / self.nobs)\n",
    "        return dffits_, dffits_threshold\n",
    "\n",
    "    @cache_readonly\n",
    "    def dffits(self):\n",
    "        '''(cached attribute) dffits measure for influence of an observation\n",
    "\n",
    "        based on resid_studentized_external,\n",
    "        uses results from leave-one-observation-out loop\n",
    "\n",
    "        It is recommended that observations with dffits large than a\n",
    "        threshold of 2 sqrt{k / n} where k is the number of parameters, should\n",
    "        be investigated.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dffits: float\n",
    "        dffits_threshold : float\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        `Wikipedia <http://en.wikipedia.org/wiki/DFFITS>`_\n",
    "\n",
    "        '''\n",
    "        #TODO: do I want to use different sigma estimate in\n",
    "        #      resid_studentized_external\n",
    "        # -> move definition of sigma_error to the __init__\n",
    "        hii = self.hat_matrix_diag\n",
    "        dffits_ = self.resid_studentized_external * np.sqrt(hii / (1 - hii))\n",
    "        dffits_threshold = 2 * np.sqrt(self.k_vars * 1. / self.nobs)\n",
    "        return dffits_, dffits_threshold\n",
    "\n",
    "    @cache_readonly\n",
    "    def dfbetas(self):\n",
    "        '''(cached attribute) dfbetas\n",
    "\n",
    "        uses results from leave-one-observation-out loop\n",
    "        '''\n",
    "        dfbetas = self.results.params - self.params_not_obsi#[None,:]\n",
    "        dfbetas /= np.sqrt(self.sigma2_not_obsi[:,None])\n",
    "        dfbetas /=  np.sqrt(np.diag(self.results.normalized_cov_params))\n",
    "        return dfbetas\n",
    "\n",
    "    @cache_readonly\n",
    "    def sigma2_not_obsi(self):\n",
    "        '''(cached attribute) error variance for all LOOO regressions\n",
    "\n",
    "        This is 'mse_resid' from each auxiliary regression.\n",
    "\n",
    "        uses results from leave-one-observation-out loop\n",
    "        '''\n",
    "        return np.asarray(self._res_looo['mse_resid'])\n",
    "\n",
    "    @cache_readonly\n",
    "    def params_not_obsi(self):\n",
    "        '''(cached attribute) parameter estimates for all LOOO regressions\n",
    "\n",
    "        uses results from leave-one-observation-out loop\n",
    "        '''\n",
    "        return np.asarray(self._res_looo['params'])\n",
    "\n",
    "    @cache_readonly\n",
    "    def det_cov_params_not_obsi(self):\n",
    "        '''(cached attribute) determinant of cov_params of all LOOO regressions\n",
    "\n",
    "        uses results from leave-one-observation-out loop\n",
    "        '''\n",
    "        return np.asarray(self._res_looo['det_cov_params'])\n",
    "\n",
    "    @cache_readonly\n",
    "    def cooks_distance(self):\n",
    "        '''(cached attribute) Cooks distance\n",
    "\n",
    "        uses original results, no nobs loop\n",
    "\n",
    "        '''\n",
    "        hii = self.hat_matrix_diag\n",
    "        #Eubank p.93, 94\n",
    "        cooks_d2 = self.resid_studentized_internal**2 / self.k_vars\n",
    "        cooks_d2 *= hii / (1 - hii)\n",
    "\n",
    "        from scipy import stats\n",
    "        #alpha = 0.1\n",
    "        #print stats.f.isf(1-alpha, n_params, res.df_modelwc)\n",
    "        pvals = stats.f.sf(cooks_d2, self.k_vars, self.results.df_resid)\n",
    "\n",
    "        return cooks_d2, pvals\n",
    "\n",
    "    @cache_readonly\n",
    "    def cov_ratio(self):\n",
    "        '''(cached attribute) covariance ratio between LOOO and original\n",
    "\n",
    "        This uses determinant of the estimate of the parameter covariance\n",
    "        from leave-one-out estimates.\n",
    "        requires leave one out loop for observations\n",
    "\n",
    "        '''\n",
    "        #don't use inplace division / because then we change original\n",
    "        cov_ratio = (self.det_cov_params_not_obsi\n",
    "                            / np.linalg.det(self.results.cov_params()))\n",
    "        return cov_ratio\n",
    "\n",
    "    @cache_readonly\n",
    "    def resid_var(self):\n",
    "        '''(cached attribute) estimate of variance of the residuals\n",
    "\n",
    "        ::\n",
    "\n",
    "           sigma2 = sigma2_OLS * (1 - hii)\n",
    "\n",
    "        where hii is the diagonal of the hat matrix\n",
    "\n",
    "        '''\n",
    "        #TODO:check if correct outside of ols\n",
    "        return self.results.mse_resid * (1 - self.hat_matrix_diag)\n",
    "\n",
    "    @cache_readonly\n",
    "    def resid_std(self):\n",
    "        '''(cached attribute) estimate of standard deviation of the residuals\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        resid_var\n",
    "\n",
    "        '''\n",
    "        return np.sqrt(self.resid_var)\n",
    "\n",
    "\n",
    "    def _ols_xnoti(self, drop_idx, endog_idx='endog', store=True):\n",
    "        '''regression results from LOVO auxiliary regression with cache\n",
    "\n",
    "\n",
    "        The result instances are stored, which could use a large amount of\n",
    "        memory if the datasets are large. There are too many combinations to\n",
    "        store them all, except for small problems.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        drop_idx : int\n",
    "            index of exog that is dropped from the regression\n",
    "        endog_idx : 'endog' or int\n",
    "            If 'endog', then the endogenous variable of the result instance\n",
    "            is regressed on the exogenous variables, excluding the one at\n",
    "            drop_idx. If endog_idx is an integer, then the exog with that\n",
    "            index is regressed with OLS on all other exogenous variables.\n",
    "            (The latter is the auxiliary regression for the variance inflation\n",
    "            factor.)\n",
    "\n",
    "        this needs more thought, memory versus speed\n",
    "        not yet used in any other parts, not sufficiently tested\n",
    "        '''\n",
    "        #reverse the structure, access store, if fail calculate ?\n",
    "        #this creates keys in store even if store = false ! bug\n",
    "        if endog_idx == 'endog':\n",
    "            stored = self.aux_regression_endog\n",
    "            if hasattr(stored, drop_idx):\n",
    "                return stored[drop_idx]\n",
    "            x_i = self.results.model.endog\n",
    "\n",
    "        else:\n",
    "            #nested dictionary\n",
    "            try:\n",
    "                self.aux_regression_exog[endog_idx][drop_idx]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "            stored = self.aux_regression_exog[endog_idx]\n",
    "            stored = {}\n",
    "\n",
    "            x_i = self.exog[:, endog_idx]\n",
    "\n",
    "        k_vars = self.exog.shape[1]\n",
    "        mask = np.arange(k_vars) != drop_idx\n",
    "        x_noti = self.exog[:, mask]\n",
    "        res = OLS(x_i, x_noti).fit()\n",
    "        if store:\n",
    "            stored[drop_idx] = res\n",
    "\n",
    "        return res\n",
    "\n",
    "    def _get_drop_vari(self, attributes):\n",
    "        '''regress endog on exog without one of the variables\n",
    "\n",
    "        This uses a k_vars loop, only attributes of the OLS instance are stored.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        attributes : list of strings\n",
    "           These are the names of the attributes of the auxiliary OLS results\n",
    "           instance that are stored and returned.\n",
    "\n",
    "        not yet used\n",
    "        '''\n",
    "        from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n",
    "\n",
    "        endog = self.results.model.endog\n",
    "        exog = self.exog\n",
    "\n",
    "        cv_iter = LeaveOneOut(self.k_vars)\n",
    "        res_loo = defaultdict(list)\n",
    "        for inidx, outidx in cv_iter:\n",
    "            for att in attributes:\n",
    "                res_i = self.model_class(endog, exog[:,inidx]).fit()\n",
    "                res_loo[att].append(getattr(res_i, att))\n",
    "\n",
    "        return res_loo\n",
    "\n",
    "    @cache_readonly\n",
    "    def _res_looo(self):\n",
    "        '''collect required results from the LOOO loop\n",
    "\n",
    "        all results will be attached.\n",
    "        currently only 'params', 'mse_resid', 'det_cov_params' are stored\n",
    "\n",
    "        regresses endog on exog dropping one observation at a time\n",
    "\n",
    "        this uses a nobs loop, only attributes of the OLS instance are stored.\n",
    "        '''\n",
    "        from statsmodels.sandbox.tools.cross_val import LeaveOneOut\n",
    "        get_det_cov_params = lambda res: np.linalg.det(res.cov_params())\n",
    "\n",
    "        endog = self.endog\n",
    "        exog = self.exog\n",
    "\n",
    "        params = np.zeros_like(exog)\n",
    "        mse_resid = np.zeros_like(endog)\n",
    "        det_cov_params = np.zeros_like(endog)\n",
    "\n",
    "        cv_iter = LeaveOneOut(self.nobs)\n",
    "        for inidx, outidx in cv_iter:\n",
    "            res_i = self.model_class(endog[inidx], exog[inidx]).fit()\n",
    "            params[outidx] = res_i.params\n",
    "            mse_resid[outidx] = res_i.mse_resid\n",
    "            det_cov_params[outidx] = get_det_cov_params(res_i)\n",
    "\n",
    "        return dict(params=params, mse_resid=mse_resid,\n",
    "                       det_cov_params=det_cov_params)\n",
    "\n",
    "    def summary_frame(self):\n",
    "        \"\"\"\n",
    "        Creates a DataFrame with all available influence results.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        frame : DataFrame\n",
    "            A DataFrame with all results.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The resultant DataFrame contains six variables in addition to the\n",
    "        DFBETAS. These are:\n",
    "\n",
    "        * cooks_d : Cook's Distance defined in `Influence.cooks_distance`\n",
    "        * standard_resid : Standardized residuals defined in\n",
    "          `Influence.resid_studentized_internal`\n",
    "        * hat_diag : The diagonal of the projection, or hat, matrix defined in\n",
    "          `Influence.hat_matrix_diag`\n",
    "        * dffits_internal : DFFITS statistics using internally Studentized\n",
    "          residuals defined in `Influence.dffits_internal`\n",
    "        * dffits : DFFITS statistics using externally Studentized residuals\n",
    "          defined in `Influence.dffits`\n",
    "        * student_resid : Externally Studentized residuals defined in\n",
    "          `Influence.resid_studentized_external`\n",
    "        \"\"\"\n",
    "        from pandas import DataFrame\n",
    "\n",
    "        # row and column labels\n",
    "        data = self.results.model.data\n",
    "        row_labels = data.row_labels\n",
    "        beta_labels = ['dfb_' + i for i in data.xnames]\n",
    "\n",
    "        # grab the results\n",
    "        summary_data = DataFrame(dict(\n",
    "                            cooks_d = self.cooks_distance[0],\n",
    "                            standard_resid = self.resid_studentized_internal,\n",
    "                            hat_diag = self.hat_matrix_diag,\n",
    "                            dffits_internal = self.dffits_internal[0],\n",
    "                            student_resid = self.resid_studentized_external,\n",
    "                            dffits = self.dffits[0],\n",
    "                                        ),\n",
    "                            index = row_labels)\n",
    "        #NOTE: if we don't give columns, order of above will be arbitrary\n",
    "        dfbeta = DataFrame(self.dfbetas, columns=beta_labels,\n",
    "                            index=row_labels)\n",
    "\n",
    "        return dfbeta.join(summary_data)\n",
    "\n",
    "    def summary_table(self, float_fmt=\"%6.3f\"):\n",
    "        '''create a summary table with all influence and outlier measures\n",
    "\n",
    "        This does currently not distinguish between statistics that can be\n",
    "        calculated from the original regression results and for which a\n",
    "        leave-one-observation-out loop is needed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        res : SimpleTable instance\n",
    "           SimpleTable instance with the results, can be printed\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This also attaches table_data to the instance.\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        #print self.dfbetas\n",
    "\n",
    "#        table_raw = [ np.arange(self.nobs),\n",
    "#                      self.endog,\n",
    "#                      self.fittedvalues,\n",
    "#                      self.cooks_distance(),\n",
    "#                      self.resid_studentized_internal,\n",
    "#                      self.hat_matrix_diag,\n",
    "#                      self.dffits_internal,\n",
    "#                      self.resid_studentized_external,\n",
    "#                      self.dffits,\n",
    "#                      self.dfbetas\n",
    "#                      ]\n",
    "        table_raw = [ ('obs', np.arange(self.nobs)),\n",
    "                      ('endog', self.endog),\n",
    "                      ('fitted\\nvalue', self.results.fittedvalues),\n",
    "                      (\"Cook's\\nd\", self.cooks_distance[0]),\n",
    "                      (\"student.\\nresidual\", self.resid_studentized_internal),\n",
    "                      ('hat diag', self.hat_matrix_diag),\n",
    "                      ('dffits \\ninternal', self.dffits_internal[0]),\n",
    "                      (\"ext.stud.\\nresidual\", self.resid_studentized_external),\n",
    "                      ('dffits', self.dffits[0])\n",
    "                      ]\n",
    "        colnames, data = lzip(*table_raw) #unzip\n",
    "        data = np.column_stack(data)\n",
    "        self.table_data = data\n",
    "        from statsmodels.iolib.table import SimpleTable, default_html_fmt\n",
    "        from statsmodels.iolib.tableformatting import fmt_base\n",
    "        from copy import deepcopy\n",
    "        fmt = deepcopy(fmt_base)\n",
    "        fmt_html = deepcopy(default_html_fmt)\n",
    "        fmt['data_fmts'] = [\"%4d\"] + [float_fmt] * (data.shape[1] - 1)\n",
    "        #fmt_html['data_fmts'] = fmt['data_fmts']\n",
    "        return SimpleTable(data, headers=colnames, txt_fmt=fmt,\n",
    "                           html_fmt=fmt_html)\n",
    "\n",
    "\n",
    "def summary_table(res, alpha=0.05):\n",
    "    '''generate summary table of outlier and influence similar to SAS\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float\n",
    "       significance level for confidence interval\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    st : SimpleTable instance\n",
    "       table with results that can be printed\n",
    "    data : ndarray\n",
    "       calculated measures and statistics for the table\n",
    "    ss2 : list of strings\n",
    "       column_names for table (Note: rows of table are observations)\n",
    "\n",
    "    '''\n",
    "\n",
    "    from scipy import stats\n",
    "    from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "\n",
    "    infl = OLSInfluence(res)\n",
    "\n",
    "    #standard error for predicted mean\n",
    "    #Note: using hat_matrix only works for fitted values\n",
    "    predict_mean_se = np.sqrt(infl.hat_matrix_diag*res.mse_resid)\n",
    "\n",
    "    tppf = stats.t.isf(alpha/2., res.df_resid)\n",
    "    predict_mean_ci = np.column_stack([\n",
    "                        res.fittedvalues - tppf * predict_mean_se,\n",
    "                        res.fittedvalues + tppf * predict_mean_se])\n",
    "\n",
    "\n",
    "    #standard error for predicted observation\n",
    "    predict_se, predict_ci_low, predict_ci_upp = wls_prediction_std(res)\n",
    "    predict_ci = np.column_stack((predict_ci_low, predict_ci_upp))\n",
    "\n",
    "    #standard deviation of residual\n",
    "    resid_se = np.sqrt(res.mse_resid * (1 - infl.hat_matrix_diag))\n",
    "\n",
    "    table_sm = np.column_stack([\n",
    "                                  np.arange(res.nobs) + 1,\n",
    "                                  res.model.endog,\n",
    "                                  res.fittedvalues,\n",
    "                                  predict_mean_se,\n",
    "                                  predict_mean_ci[:,0],\n",
    "                                  predict_mean_ci[:,1],\n",
    "                                  predict_ci[:,0],\n",
    "                                  predict_ci[:,1],\n",
    "                                  res.resid,\n",
    "                                  resid_se,\n",
    "                                  infl.resid_studentized_internal,\n",
    "                                  infl.cooks_distance[0]\n",
    "                                  ])\n",
    "\n",
    "\n",
    "    #colnames, data = lzip(*table_raw) #unzip\n",
    "    data = table_sm\n",
    "    ss2 = ['Obs', 'Dep Var\\nPopulation', 'Predicted\\nValue', 'Std Error\\nMean Predict', 'Mean ci\\n95% low', 'Mean ci\\n95% upp', 'Predict ci\\n95% low', 'Predict ci\\n95% upp', 'Residual', 'Std Error\\nResidual', 'Student\\nResidual', \"Cook's\\nD\"]\n",
    "    colnames = ss2\n",
    "    #self.table_data = data\n",
    "    #data = np.column_stack(data)\n",
    "    from statsmodels.iolib.table import SimpleTable, default_html_fmt\n",
    "    from statsmodels.iolib.tableformatting import fmt_base\n",
    "    from copy import deepcopy\n",
    "    fmt = deepcopy(fmt_base)\n",
    "    fmt_html = deepcopy(default_html_fmt)\n",
    "    fmt['data_fmts'] = [\"%4d\"] + [\"%6.3f\"] * (data.shape[1] - 1)\n",
    "    #fmt_html['data_fmts'] = fmt['data_fmts']\n",
    "    st = SimpleTable(data, headers=colnames, txt_fmt=fmt,\n",
    "                       html_fmt=fmt_html)\n",
    "\n",
    "    return st, data, ss2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
